{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqIQd4AjNfzh",
        "outputId": "486a5654-773d-474a-d48d-bc55b654fb59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks\n",
        "\n",
        "os.chdir('./OCTA_CycleGAN/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPoRdq-gWrdb",
        "outputId": "5b568f09-a6e1-43d0-91e6-a5931dd6378f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 389 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 419 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 450 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 471 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 481 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 501 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 512 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 532 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 563 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 583 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 624 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 645 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 665 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 676 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.1.2)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=face17616a3948a8db4e17bbb5188f5efb6e4f9a919049d390a24772207c377a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5710 sha256=56a1637af22326bebd6ccdd9922d352d8111ed3d9f513fc998e3b711025c2a40\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: jsonpointer, websocket-client, torchfile, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.32 jsonpointer-2.2 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-1.2.3\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.19.5)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.62.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->lpips) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (7.1.2)\n",
            "Installing collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ],
      "source": [
        "# Prepare package\n",
        "!pip install visdom\n",
        "!pip install lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqtDwDeK6uwf",
        "outputId": "e74daa3c-bee6-4866-b4fb-c0dd783f4c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/OCTA_CycleGAN/pytorch_wavelets\n",
            "Processing /content/drive/MyDrive/Colab Notebooks/OCTA_CycleGAN/pytorch_wavelets\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-wavelets==1.3.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pytorch-wavelets==1.3.0) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-wavelets==1.3.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-wavelets==1.3.0) (3.10.0.2)\n",
            "Building wheels for collected packages: pytorch-wavelets\n",
            "  Building wheel for pytorch-wavelets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-wavelets: filename=pytorch_wavelets-1.3.0-py3-none-any.whl size=54869 sha256=7f1c6c1686e6de45da605e6f1f96f49ccfa48622c21590742ac132c4593377d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-73p9_vae/wheels/82/1f/1d/df88cea24a9de9a259b29c50aa658dd7e6ed94eb3b6b6d3152\n",
            "Successfully built pytorch-wavelets\n",
            "Installing collected packages: pytorch-wavelets\n",
            "Successfully installed pytorch-wavelets-1.3.0\n",
            "/content/drive/MyDrive/Colab Notebooks/OCTA_CycleGAN\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/fbcotter/pytorch_wavelets\n",
        "%cd ./pytorch_wavelets/\n",
        "!pip install .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "2cfe4c0e4f9a47da8d71b7f938045221"
          ]
        },
        "id": "qjL8Q3lIPE3z",
        "outputId": "fed0e4b1-ae75-4d54-8924-fbc419b0c934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cfe4c0e4f9a47da8d71b7f938045221",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from visdom import Visdom\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pytorch_wavelets import DWTForward\n",
        "\n",
        "import pdb\n",
        "import skimage.metrics\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lpips\n",
        "loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
        "# loss_fn_vgg = lpips.LPIPS(net='vgg') # closer to \"traditional\" perceptual loss, when used for optimization\n",
        "\n",
        "# import pytorch_fft.fft.autograd as fft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FcgHTBToWfrz"
      },
      "outputs": [],
      "source": [
        "# Helper Functions\n",
        "def tensor2image(tensor):\n",
        "    image = 127.5*(tensor[0].cpu().float().numpy() + 1.0)\n",
        "    if image.shape[0] == 1:\n",
        "        image = np.tile(image, (3,1,1))\n",
        "    return image.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size=50):\n",
        "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                if random.uniform(0,1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size-1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = element\n",
        "                else:\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))\n",
        "\n",
        "class LambdaLR():\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)\n",
        "\n",
        "def guais_low_pass(img, radius=10):\n",
        "    rows, cols = img.shape\n",
        "    center = int(rows / 2), int(cols / 2)\n",
        "\n",
        "    mask = np.zeros((rows, cols))\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            distance_u_v = (i - center[0]) ** 2 + (j - center[1]) ** 2\n",
        "            mask[i, j] = np.exp(-0.5 *  distance_u_v / (radius ** 2))\n",
        "    return torch.from_numpy(mask).float()\n",
        "\n",
        "\n",
        "def guais_high_pass(img, radius=10):\n",
        "    rows, cols = img.shape\n",
        "    center = int(rows / 2), int(cols / 2)\n",
        "\n",
        "    mask = np.zeros((rows, cols))\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            distance_u_v = (i - center[0]) ** 2 + (j - center[1]) ** 2\n",
        "            mask[i, j] = 1 - np.exp(-0.5 *  distance_u_v / (radius ** 2))\n",
        "    return torch.from_numpy(mask).float()\n",
        "\n",
        "\n",
        "def high_pass(timg, i=4):\n",
        "    f = torch.fft.fft2(timg[0])\n",
        "    fshift = torch.fft.fftshift(f)\n",
        "    \n",
        "    # rows, cols = timg[0].shape\n",
        "    # crow, ccol = int(rows/2), int(cols/2)\n",
        "    # fshift[crow-i:crow+i, ccol-i:ccol+i] = 0\n",
        "    mask = guais_high_pass(fshift, i).cuda()\n",
        "    f = fshift * mask\n",
        "\n",
        "    ishift = torch.fft.ifftshift(f)\n",
        "    iimg = torch.fft.ifft2(ishift)\n",
        "    iimg = torch.abs(iimg)\n",
        "    return iimg\n",
        "\n",
        "def low_pass(timg, i=10):\n",
        "    f = torch.fft.fft2(timg[0])\n",
        "    fshift = torch.fft.fftshift(f)\n",
        "    \n",
        "    # print(timg[0].shape)\n",
        "    rows, cols = timg[0].shape\n",
        "    \n",
        "    crow,ccol = int(rows/2), int(cols/2)\n",
        "    # mask = torch.zeros((rows, cols)).cuda()\n",
        "    # mask[crow-i:crow+i, ccol-i:ccol+i] = 1\n",
        "    mask = guais_low_pass(fshift, i).cuda()\n",
        "    \n",
        "    \n",
        "    f = fshift * mask\n",
        "    \n",
        "    ishift = torch.fft.ifftshift(f)\n",
        "    iimg = torch.fft.ifft2(ishift)\n",
        "    iimg = torch.abs(iimg)\n",
        "    return iimg*-1\n",
        "\n",
        "def bandreject_pass(timg, r_out=300, r_in=35):\n",
        "    f = torch.fft.fft2(timg[0])\n",
        "    fshift = torch.fft.fftshift(f)\n",
        "    \n",
        "    rows, cols = timg[0].shape\n",
        "    crow,ccol = int(rows/2), int(cols/2)\n",
        "    # mask = torch.zeros((rows, cols)).cuda()\n",
        "    # mask[crow-i:crow+i, ccol-i:ccol+i] = 1\n",
        "    mask = bandreject_filters(fshift, r_out, r_in).cuda()\n",
        "    \n",
        "    f = fshift * mask\n",
        "    \n",
        "    ishift = torch.fft.ifftshift(f)\n",
        "    iimg = torch.fft.ifft2(ishift)\n",
        "    iimg = torch.abs(iimg)\n",
        "    return iimg\n",
        "\n",
        "def bandreject_filters(img, r_out=300, r_in=35):\n",
        "    rows, cols = img.shape\n",
        "    crow, ccol = int(rows / 2), int(cols / 2)\n",
        "\n",
        "    radius_out = r_out\n",
        "    radius_in = r_in\n",
        "\n",
        "    mask = np.zeros((rows, cols))\n",
        "    center = [crow, ccol]\n",
        "    x, y = np.ogrid[:rows, :cols]\n",
        "    mask_area = np.logical_and(((x - center[0]) ** 2 + (y - center[1]) ** 2 >= r_in ** 2),\n",
        "                               ((x - center[0]) ** 2 + (y - center[1]) ** 2 <= r_out ** 2))\n",
        "    mask[mask_area] = 1\n",
        "    return torch.from_numpy(mask).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VpKdVydJRpRB"
      },
      "outputs": [],
      "source": [
        "class ImagePool():\n",
        "    \"\"\"This class implements an image buffer that stores previously generated images.\n",
        "\n",
        "    This buffer enables us to update discriminators using a history of generated images\n",
        "    rather than the ones produced by the latest generators.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pool_size):\n",
        "        \"\"\"Initialize the ImagePool class\n",
        "\n",
        "        Parameters:\n",
        "            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n",
        "        \"\"\"\n",
        "        self.pool_size = pool_size\n",
        "        if self.pool_size > 0:  # create an empty pool\n",
        "            self.num_imgs = 0\n",
        "            self.images = []\n",
        "\n",
        "    def query(self, images):\n",
        "        \"\"\"Return an image from the pool.\n",
        "\n",
        "        Parameters:\n",
        "            images: the latest generated images from the generator\n",
        "\n",
        "        Returns images from the buffer.\n",
        "\n",
        "        By 50/100, the buffer will return input images.\n",
        "        By 50/100, the buffer will return images previously stored in the buffer,\n",
        "        and insert the current images to the buffer.\n",
        "        \"\"\"\n",
        "        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n",
        "            return images\n",
        "        return_images = []\n",
        "        for image in images:\n",
        "            image = torch.unsqueeze(image.data, 0)\n",
        "            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n",
        "                self.num_imgs = self.num_imgs + 1\n",
        "                self.images.append(image)\n",
        "                return_images.append(image)\n",
        "            else:\n",
        "                p = random.uniform(0, 1)\n",
        "                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n",
        "                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n",
        "                    tmp = self.images[random_id].clone()\n",
        "                    self.images[random_id] = image\n",
        "                    return_images.append(tmp)\n",
        "                else:       # by another 50% chance, the buffer will return the current image\n",
        "                    return_images.append(image)\n",
        "        return_images = torch.cat(return_images, 0)   # collect all the images and return\n",
        "        return return_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LZPM5OT8PWXC"
      },
      "outputs": [],
      "source": [
        "# Dataloader\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_A=None, transforms_B=None, unaligned=False, mode='train'):\n",
        "        self.transformA = transforms.Compose(transforms_A)\n",
        "        self.transformB = transforms.Compose(transforms_B)\n",
        "\n",
        "        self.unaligned = unaligned\n",
        "\n",
        "        self.files_A = sorted(glob.glob(os.path.join(root, 'trainA') + '/*.*'))\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root, 'trainB') + '/*.*'))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_A = Image.open(self.files_A[index % len(self.files_A)]).convert('L')\n",
        "        item_A = self.transformA(img_A)\n",
        "\n",
        "        if self.unaligned:\n",
        "            item_B = self.transformB(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]).convert('L'))\n",
        "        else:\n",
        "            item_B = self.transformB(Image.open(self.files_B[index % len(self.files_B)]).convert('L'))\n",
        "\n",
        "        return {'A': item_A, 'B': item_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sEDL-1Gldu83"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc=1, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        use_bias = True\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "irviOIQfRUR2"
      },
      "outputs": [],
      "source": [
        "def laplacian_kernel(im):\n",
        "    conv_op = nn.Conv2d(1, 1, 3, bias=False, stride=1, padding=1)\n",
        "    laplacian_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32')\n",
        "    laplacian_kernel = laplacian_kernel.reshape((1, 1, 3, 3))\n",
        "    conv_op.weight.data = torch.from_numpy(laplacian_kernel).cuda()\n",
        "    edge_detect = conv_op(Variable(im))\n",
        "    return edge_detect\n",
        "\n",
        "\n",
        "\n",
        "def functional_conv2d(im):\n",
        "    sobel_kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype='float32')  #\n",
        "    sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3))\n",
        "    weight = Variable(torch.from_numpy(sobel_kernel))\n",
        "    edge_detect = F.conv2d(Variable(im), weight)\n",
        "    edge_detect = edge_detect.squeeze().detach().numpy()\n",
        "    return edge_detect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UUf4-vPfUv64"
      },
      "outputs": [],
      "source": [
        "def set_requires_grad(nets, requires_grad=False):\n",
        "    \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
        "    Parameters:\n",
        "        nets (network list)   -- a list of networks\n",
        "        requires_grad (bool)  -- whether the networks require gradients or not\n",
        "    \"\"\"\n",
        "    if not isinstance(nets, list):\n",
        "        nets = [nets]\n",
        "    for net in nets:\n",
        "        if net is not None:\n",
        "            for param in net.parameters():\n",
        "                param.requires_grad = requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MIVdBTyt3iR_"
      },
      "outputs": [],
      "source": [
        "#################################################################\n",
        "################# Frequency Discriminator ####################\n",
        "#################################################################\n",
        "class FS_DiscriminatorA(nn.Module):\n",
        "    def __init__(self, recursions=1, stride=1, kernel_size=5, wgan=False, highpass=True, D_arch='FSD',\n",
        "                 norm_layer='Instance', filter_type='gau', cs='sum'):\n",
        "        super(FS_DiscriminatorA, self).__init__()\n",
        "\n",
        "        self.wgan = wgan\n",
        "        n_input_channel = 1\n",
        "        \n",
        "        self.DWT2 = DWTForward(J=1, wave='haar', mode='reflect')\n",
        "        self.filter = self.filter_wavelet\n",
        "        self.cs = cs\n",
        "\n",
        "        print('# FS type: {}, kernel size={}'.format(filter_type.lower(), kernel_size))\n",
        "        \n",
        "\n",
        "        self.net = Discriminator(input_nc=1)\n",
        "        if cs=='sum':\n",
        "          self.net_dwt = Discriminator(input_nc=1)\n",
        "        else:\n",
        "          self.net_dwt = Discriminator(input_nc=3)\n",
        "        self.attention = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Conv2d(128, 64 , 1, 1),\n",
        "                nn.Conv2d(64 , 128, 1, 1),\n",
        "                nn.Sigmoid())\n",
        "        self.out_net = nn.Softmax()\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        dwt, ximg = self.filter(x)\n",
        "        # LL, LH, HL, HH, ximg = self.filter(x)\n",
        "        x = self.net(ximg)\n",
        "        x_D = F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "        dwt_D = self.net_dwt(dwt)\n",
        "        dwt_D = F.avg_pool2d(dwt_D, dwt_D.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "\n",
        "        # img_LL = self.net(LL)\n",
        "        # img_LL = F.avg_pool2d(img_LL, img_LL.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "        # img_LH = self.net(LH)\n",
        "        # img_LH = F.avg_pool2d(img_LH, img_LH.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "        # img_HL = self.net(HL)\n",
        "        # img_HL = F.avg_pool2d(img_HL, img_HL.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "        # img_HH = self.net(HH)\n",
        "        # img_HH = F.avg_pool2d(img_HH, img_HH.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "        # return x_D\n",
        "        return (torch.flatten(0.8*x_D + 0.2*dwt_D))\n",
        "\n",
        "    def filter_wavelet(self, x, norm=True):\n",
        "        LL, Hc = self.DWT2(x)\n",
        "        LH, HL, HH = Hc[0][:, :, 0, :, :], Hc[0][:, :, 1, :, :], Hc[0][:, :, 2, :, :]\n",
        "        if norm:\n",
        "            LH, HL, HH = LH * 0.5 + 0.5, HL * 0.5 + 0.5, HH * 0.5 + 0.5\n",
        "        if self.cs.lower() == 'sum':\n",
        "            return LL, x\n",
        "\n",
        "        elif self.cs.lower() == 'each':\n",
        "            return LL, LH, HL, HH, x\n",
        "        elif self.cs.lower() == 'cat':\n",
        "            return torch.cat((LH, HL, HH), 1), x\n",
        "        else:\n",
        "            raise NotImplementedError('Wavelet format [{:s}] not recognized'.format(self.cs))\n",
        "\n",
        "\n",
        "class FS_DiscriminatorB(nn.Module):\n",
        "    def __init__(self, recursions=1, stride=1, kernel_size=5, wgan=False, highpass=True, D_arch='FSD',\n",
        "                 norm_layer='Instance', filter_type='gau', cs='cat'):\n",
        "        super(FS_DiscriminatorB, self).__init__()\n",
        "\n",
        "        self.wgan = wgan\n",
        "        n_input_channel = 1\n",
        "        \n",
        "        self.DWT2 = DWTForward(J=1, wave='haar', mode='reflect')\n",
        "        self.filter = self.filter_wavelet\n",
        "        self.cs = cs\n",
        "        # n_input_channel = 3\n",
        "        n_input_channel = 1\n",
        "\n",
        "        print('# FS type: {}, kernel size={}'.format(filter_type.lower(), kernel_size))\n",
        "        \n",
        "\n",
        "        self.net = Discriminator(input_nc=1)\n",
        "        if cs=='sum':\n",
        "          self.net_dwt = Discriminator(input_nc=1)\n",
        "        else:\n",
        "          self.net_dwt = Discriminator(input_nc=3)\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Conv2d(128, 64 , 1, 1),\n",
        "                nn.Conv2d(64 , 128, 1, 1),\n",
        "                nn.Sigmoid())\n",
        "        self.out_net = nn.Softmax()\n",
        "\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        dwt, ximg = self.filter(x)\n",
        "        # LL, LH, HL, HH, ximg = self.filter(x)\n",
        "        x = self.net(ximg)\n",
        "        x_D = F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "        dwt_D = self.net_dwt(dwt)\n",
        "        dwt_D = F.avg_pool2d(dwt_D, dwt_D.size()[2:]).view(x.size()[0], -1)\n",
        "\n",
        "        # return x_D\n",
        "        return (torch.flatten(0.8*x_D + 0.2*dwt_D))\n",
        "\n",
        "    def filter_wavelet(self, x, norm=True):\n",
        "        LL, Hc = self.DWT2(x)\n",
        "        LH, HL, HH = Hc[0][:, :, 0, :, :], Hc[0][:, :, 1, :, :], Hc[0][:, :, 2, :, :]\n",
        "        if norm:\n",
        "            LH, HL, HH = LH * 0.5 + 0.5, HL * 0.5 + 0.5, HH * 0.5 + 0.5\n",
        "        if self.cs.lower() == 'sum':\n",
        "            return HH, x\n",
        "\n",
        "        elif self.cs.lower() == 'each':\n",
        "            return LL, LH, HL, HH, x\n",
        "        elif self.cs.lower() == 'cat':\n",
        "            return torch.cat((LH, HL, HH), 1), x\n",
        "        else:\n",
        "            raise NotImplementedError('Wavelet format [{:s}] not recognized'.format(self.cs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6CIIbmIMnnyl"
      },
      "outputs": [],
      "source": [
        "def save_sample(epoch, tensor, suffix=\"_real\"):\n",
        "    output = tensor.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "    plt.imsave('./checkpoint/image_alt_'+str(epoch+1)+suffix+'.jpeg', output, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OplFbqdpkcHH"
      },
      "outputs": [],
      "source": [
        "class phase_consistency_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "        # print(x.size())\n",
        "        radius = 5\n",
        "        rows, cols = x[0][0].shape\n",
        "        center = int(rows / 2), int(cols / 2)\n",
        "\n",
        "        mask = np.zeros((rows, cols))\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                distance_u_v = (i - center[0]) ** 2 + (j - center[1]) ** 2\n",
        "                mask[i, j] = 1 - np.exp(-0.5 *  distance_u_v / (radius ** 2))\n",
        "        m = torch.from_numpy(mask).float().cuda()\n",
        "\n",
        "        f_x = torch.fft.fft2(x[0])\n",
        "        fshift_x = torch.fft.fftshift(f_x)\n",
        "        amp_x = (m * torch.log(torch.abs(fshift_x))).flatten()\n",
        "        # print(y.size())\n",
        "        f_y = torch.fft.fft2(y[0])\n",
        "        fshift_y = torch.fft.fftshift(f_y)\n",
        "        amp_y = (m * torch.log(torch.abs(fshift_y))).flatten()\n",
        "        # print(amp_x.size(), amp_y.size())\n",
        "        return -torch.cosine_similarity(amp_x, amp_y, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZXf69PfiRuSj"
      },
      "outputs": [],
      "source": [
        "#### Defination of local variables\n",
        "input_nc = 1\n",
        "output_nc = 1\n",
        "batchSize = 1\n",
        "size_A, size_B = 128, 256\n",
        "lr = 2e-4\n",
        "n_epochs, epoch, decay_epoch = 60, 0, 10\n",
        "n_cpu = 2\n",
        "dataroot = \"./dataset/Colab_random_OCTA_augmented\"\n",
        "cuda = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Kb1cpw_vP70j"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available() and not cuda:\n",
        "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8mzWZg9oedqC"
      },
      "outputs": [],
      "source": [
        "class UnetGeneratorB2A(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, num_downs=5, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetGeneratorB2A, self).__init__()\n",
        "\n",
        "        ####################### UNet #######################\n",
        "        ######################################################\n",
        "        use_bias = False\n",
        "        # (1, 320) -> (64, 160)\n",
        "        self.down_1 = nn.Sequential(*[nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=use_bias)])\n",
        "        # (64, 160) -> (128, 80)\n",
        "        self.down_2 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(128)])\n",
        "        # (128, 80) -> (256, 40)\n",
        "        self.down_3 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(256)])\n",
        "        # (256, 40) -> (512, 20)\n",
        "        self.down_4 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(512)])\n",
        "        # (512, 20) -> (1024, 10)\n",
        "        self.down_5 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(1024)])\n",
        "\n",
        "        # (1024, 10) -> (1024, 5)\n",
        "        self.down_6 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(1024, 1024, kernel_size=4, stride=2, padding=1, bias=use_bias)])\n",
        "        \n",
        "        ######################################################\n",
        "        # (1024, 5) -> (1024, 10)\n",
        "        self.up_6 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(1024, 1024, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(1024)])\n",
        "        # (1024, 10)*2 -> (512, 20)\n",
        "        self.up_5 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(2048, 512, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(512)])\n",
        "        # (512, 20)*2 -> (256, 40)\n",
        "        self.up_4 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(256)])\n",
        "        \n",
        "        # (256, 40)*2 -> (128, 80)\n",
        "        self.up_3 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(128)])\n",
        "        # (128, 80)*2 -> (64, 160)\n",
        "        self.up_2 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(64)])\n",
        "\n",
        "        ######################################################\n",
        "        ######################################################\n",
        "        self.shallow_frequency = nn.Sequential(*[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),\n",
        "                                      nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(64)])\n",
        "        \n",
        "        self.shallow = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1, bias=use_bias),\n",
        "                                    nn.Tanh()])\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward\"\"\"\n",
        "        hf = high_pass(input[0], i=5).unsqueeze(0).unsqueeze(0) # (1, 320) 5\n",
        "        lf = low_pass(input[0], i=14).unsqueeze(0).unsqueeze(0) # (1, 320) 14\n",
        "\n",
        "        hf_input = self.shallow_frequency(hf) # (64, 160)\n",
        "\n",
        "        down_1 = self.down_1(lf) # (64, 160)\n",
        "        down_2 = self.down_2(down_1) # (128, 80)\n",
        "        down_3 = self.down_3(down_2) # (256, 40)\n",
        "        down_4 = self.down_4(down_3) # (512, 20)\n",
        "        down_5 = self.down_5(down_4) # (1024, 10)\n",
        "        down_6 = self.down_6(down_5) # (1024, 5)\n",
        "\n",
        "        up_6 = self.up_6(down_6) # (1024, 10)\n",
        "        up_5 = self.up_5(torch.cat([down_5, up_6], 1)) # (512, 20)\n",
        "        up_4 = self.up_4(torch.cat([down_4, up_5], 1)) # (256, 40)\n",
        "        up_3 = self.up_3(torch.cat([down_3, up_4], 1)) # (128, 80)\n",
        "        up_2 = self.up_2(torch.cat([down_2, up_3], 1)) # (64, 160)\n",
        "\n",
        "        \n",
        "        return hf_input, up_2, self.shallow(up_2+hf_input) # B2A: hf_feature, lf_feature, rc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6PsIGmZB3spy"
      },
      "outputs": [],
      "source": [
        "class UnetGeneratorA2B(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, num_downs=5, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetGeneratorA2B, self).__init__()\n",
        "\n",
        "        ####################### UNet #######################\n",
        "        ######################################################\n",
        "        use_bias = False\n",
        "        # # (1, 320) -> (64, 160)\n",
        "        # self.down_1 = nn.Sequential(*[nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=True)])\n",
        "        # (64, 160) -> (128, 80)\n",
        "        self.down_1 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(128)])\n",
        "        # (128, 80) -> (256, 40)\n",
        "        self.down_2 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(256)])\n",
        "        # (256, 40) -> (512, 20)\n",
        "        self.down_3 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(512)])\n",
        "        # (512, 20) -> (1024, 10)\n",
        "        self.down_4 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(1024)])\n",
        "\n",
        "        # (1024, 10) -> (1024, 5)\n",
        "        self.down_5 = nn.Sequential(*[nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(1024, 1024, kernel_size=4, stride=2, padding=1, bias=use_bias)])\n",
        "        \n",
        "        ######################################################\n",
        "        # (1024, 5) -> (1024, 10)\n",
        "        self.up_5 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(1024, 1024, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(1024)])\n",
        "        # (1024, 10)*2 -> (512, 20)\n",
        "        self.up_4 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(2048, 512, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(512)])\n",
        "        # (512, 20)*2 -> (256, 40)\n",
        "        self.up_3 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(256)])\n",
        "        \n",
        "        # (256, 40)*2 -> (128, 80)\n",
        "        self.up_2 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(128)])\n",
        "        # (128, 80)*2 -> (64, 160)\n",
        "        self.up_1 = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.BatchNorm2d(64)])\n",
        "\n",
        "        ######################################################\n",
        "        ######################################################\n",
        "\n",
        "        self.shallow_frequency = nn.Sequential(*[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),\n",
        "                                      nn.LeakyReLU(0.2, True),\n",
        "                                      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),\n",
        "                                      nn.BatchNorm2d(64)])\n",
        "        \n",
        "        self.shallow = nn.Sequential(*[nn.ReLU(True), \n",
        "                                    nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
        "                                    nn.Tanh()])\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward\"\"\"\n",
        "        hf = high_pass(input[0], i=10).unsqueeze(0).unsqueeze(0) # (1, 320) (10 adamw) 16.22 # (gtimg+hp_lr_img)/2.0\n",
        "        hf = (hf+input)/2.0\n",
        "        lf = low_pass(input[0], i=8).unsqueeze(0).unsqueeze(0) # (1, 320) (8 adamw)\n",
        "\n",
        "        lf_input = self.shallow_frequency(lf)\n",
        "\n",
        "        hf_input = self.shallow_frequency(hf)# (64, 160)\n",
        "        down_1 = self.down_1(hf_input) # (128, 80)\n",
        "        down_2 = self.down_2(down_1) # (256, 40)\n",
        "        down_3 = self.down_3(down_2) # (512, 20)\n",
        "        down_4 = self.down_4(down_3) # (1024, 10)\n",
        "        down_5 = self.down_5(down_4) # (1024, 5)\n",
        "\n",
        "        up_5 = self.up_5(down_5) # (1024, 10)\n",
        "        up_4 = self.up_4(torch.cat([down_4, up_5], 1)) # (512, 20)\n",
        "        up_3 = self.up_3(torch.cat([down_3, up_4], 1)) # (256, 40)\n",
        "        up_2 = self.up_2(torch.cat([down_2, up_3], 1)) # (128, 80)\n",
        "        up_1 = self.up_1(torch.cat([down_1, up_2], 1)) # (64, 160)\n",
        "\n",
        "        return lf_input, hf_input, self.shallow(up_1+lf_input) # A2B: lf_feature, hf_feature, rc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xg1PyC9FKoa4"
      },
      "outputs": [],
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6aSlEi1IjSX0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XyVs_ec6rLsf"
      },
      "outputs": [],
      "source": [
        "lr_img = Image.open(\"./test/6x6_256/270_3.png\").convert('L')\n",
        "hr_img = Image.open(\"./test/3x3_256/270_6.png\").convert('L')\n",
        "T_1 = transforms.Compose([ transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5), (0.5)),\n",
        "                # transforms.Normalize((0.223), (0.142)),\n",
        "                transforms.Resize([128, 128]) ])\n",
        "T_2 = transforms.Compose([ transforms.ToTensor(),\n",
        "                # transforms.Normalize((0.223), (0.142)),                          \n",
        "                transforms.Normalize((0.5), (0.5))])\n",
        "# lr_img = cv2.resize(np.array(lr_img), dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "# lr_img = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).cuda()\n",
        "lr_img = T_1(lr_img).cuda().unsqueeze(0)\n",
        "hr_img = T_2(hr_img).cuda().unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CXu_cCqL3g1B"
      },
      "outputs": [],
      "source": [
        "def eval(model):\n",
        "  lr = \"./test/6x6_256/\"\n",
        "  hr = \"./test/3x3_256/\"\n",
        "  num, psnr, ssim = 0, 0, 0\n",
        "  T_1 = transforms.Compose([ transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.5), (0.5)),\n",
        "                      transforms.Resize([128, 128]) ])\n",
        "  T_2 = transforms.Compose([ transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.5), (0.5))])\n",
        "  for i in tqdm(range(297)):\n",
        "    lr_path = os.path.join(lr, str(i)+\"_3.png\")\n",
        "    hr_path = os.path.join(hr, str(i)+\"_6.png\")\n",
        "    if os.path.isfile(lr_path) and os.path.isfile(hr_path):\n",
        "      lr_img = Image.open(lr_path).convert('L')\n",
        "      hr_img = Image.open(hr_path).convert('L')\n",
        "      \n",
        "      lr_img = T_1(lr_img).cuda().unsqueeze(0)\n",
        "      hr_img = T_2(hr_img).cuda().unsqueeze(0)\n",
        "      \n",
        "      _, _, sr_img = model(lr_img)\n",
        "\n",
        "      yimg = sr_img.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "      gtimg = hr_img.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "      psnr += (skimage.metrics.peak_signal_noise_ratio(yimg, gtimg))\n",
        "      ssim += (skimage.metrics.structural_similarity(yimg, gtimg))\n",
        "      num += 1\n",
        "  print(\" PSNR: %.4f SSIM: %.4f\"%(psnr/num, ssim/num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eMYhch_iPt4K",
        "outputId": "08a95a9c-1b54-49f8-c4fa-57c7ecacb6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# FS type: gau, kernel size=5\n",
            "# FS type: gau, kernel size=5\n",
            "354\n"
          ]
        }
      ],
      "source": [
        "###### Definition of variables ######\n",
        "# Networks\n",
        "\n",
        "netG_A2B = UnetGeneratorA2B(output_nc, input_nc)\n",
        "netG_B2A = UnetGeneratorB2A(output_nc, input_nc)\n",
        "netD_A = FS_DiscriminatorA(input_nc)\n",
        "netD_B = FS_DiscriminatorB(output_nc)\n",
        "\n",
        "if cuda:\n",
        "    netG_A2B.cuda()\n",
        "    netG_B2A.cuda()\n",
        "    netD_A.cuda()\n",
        "    netD_B.cuda()\n",
        "\n",
        "netG_A2B.apply(weights_init_normal)\n",
        "netG_B2A.apply(weights_init_normal)\n",
        "netD_A.apply(weights_init_normal)\n",
        "netD_B.apply(weights_init_normal)\n",
        "\n",
        "# Lossess\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "# criterion_cycle = torch.nn.SmoothL1Loss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_phase = phase_consistency_loss()\n",
        "criterion_identity = torch.nn.L1Loss()\n",
        "criterion_feature = torch.nn.BCEWithLogitsLoss()\n",
        "# criterion_feature = torch.nn.KLDivLoss(size_average=False)\n",
        "# criterion_feature = torch.nn.CosineEmbeddingLoss()\n",
        "# criterion_feature = torch.nn.HingeEmbeddingLoss()\n",
        "# criterion_feature = torch.nn.MSELoss()\n",
        "# criterion_feature = torch.nn.L1Loss()\n",
        "\n",
        "# Optimizers & LR schedulers\n",
        "optimizer_G = torch.optim.AdamW(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr, betas=(0.9, 0.999))\n",
        "# optimizer_G = torch.optim.SGD(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr = lr, momentum=0.9)\n",
        "# optimizer_D_A = torch.optim.AdamW(netD_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "# optimizer_D_B = torch.optim.AdamW(netD_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "# optimizer_D = torch.optim.SGD(itertools.chain(netD_A.parameters(), netD_B.parameters()), lr = lr, momentum=0.9)\n",
        "optimizer_D = torch.optim.AdamW(itertools.chain(netD_A.parameters(), netD_B.parameters()), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n",
        "lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n",
        "# lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n",
        "# lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n",
        "\n",
        "\n",
        "\n",
        "# Inputs & targets memory allocation\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "input_A = Tensor(batchSize, input_nc, size_A, size_A)\n",
        "# input_B = Tensor(batchSize, output_nc, size_A, size_A)\n",
        "input_B = Tensor(batchSize, output_nc, size_B, size_B)\n",
        "target_real = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n",
        "target_fake = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n",
        "\n",
        "target_real = torch.flatten(target_real)\n",
        "target_fake = torch.flatten(target_fake)\n",
        "\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "\n",
        "# Dataset loader\n",
        "transforms_A = [ \n",
        "                transforms.ToTensor(),\n",
        "                # transforms.Normalize((0.246), (0.170)),\n",
        "                transforms.Normalize((0.5), (0.5)),\n",
        "                # transforms.CenterCrop(size_A),\n",
        "                transforms.RandomCrop((size_A, size_A))\n",
        "                ]\n",
        "                \n",
        "transforms_B = [ \n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5), (0.5)),\n",
        "                # transforms.Normalize((0.286), (0.200)),\n",
        "                # transforms.CenterCrop(size_B),\n",
        "                transforms.RandomCrop((size_B, size_B))\n",
        "                ]\n",
        "dataset = ImageDataset(dataroot, transforms_A=transforms_A, transforms_B=transforms_B, unaligned=True)\n",
        "print (len(dataset))\n",
        "dataloader = DataLoader(dataset, batch_size=batchSize, shuffle=True)\n",
        "\n",
        "# Loss plot\n",
        "# logger = Logger(n_epochs, len(dataloader))\n",
        "###################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TJNer1S-r63"
      },
      "source": [
        "实验:  \n",
        "(1) A2B: hf = (hf+input)/2.0 & B2A: lf = (lf+input)/2.0  \n",
        "(2) A2B: hf = (hf+input)/2.0 & B2A: lf = lf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJFSzgJwP-4q",
        "outputId": "84ca4f3f-cc0a-4095-b80c-39ef029e7990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch (1/60) Finished\n",
            "PSNR: 17.3551 SSIM: 0.4365 LPIPS: tensor([[[[0.2727]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [02:48<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.6221 SSIM: 0.4166\n",
            "Epoch (2/60) Finished\n",
            "PSNR: 16.7096 SSIM: 0.3861 LPIPS: tensor([[[[0.2393]]]])\n",
            "Epoch (3/60) Finished\n",
            "PSNR: 16.8111 SSIM: 0.4205 LPIPS: tensor([[[[0.2079]]]])\n",
            "Epoch (4/60) Finished\n",
            "PSNR: 16.7344 SSIM: 0.4002 LPIPS: tensor([[[[0.2060]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:37<00:00,  7.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.1310 SSIM: 0.3967\n",
            "Epoch (5/60) Finished\n",
            "PSNR: 16.8763 SSIM: 0.4072 LPIPS: tensor([[[[0.1917]]]])\n",
            "Epoch (6/60) Finished\n",
            "PSNR: 16.6928 SSIM: 0.4256 LPIPS: tensor([[[[0.2247]]]])\n",
            "Epoch (7/60) Finished\n",
            "PSNR: 16.6668 SSIM: 0.4100 LPIPS: tensor([[[[0.2046]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.0403 SSIM: 0.4030\n",
            "Epoch (8/60) Finished\n",
            "PSNR: 16.6015 SSIM: 0.3768 LPIPS: tensor([[[[0.1954]]]])\n",
            "Epoch (9/60) Finished\n",
            "PSNR: 16.6722 SSIM: 0.4053 LPIPS: tensor([[[[0.1939]]]])\n",
            "Epoch (10/60) Finished\n",
            "PSNR: 17.4244 SSIM: 0.4278 LPIPS: tensor([[[[0.1959]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.5655 SSIM: 0.4059\n",
            "Epoch (11/60) Finished\n",
            "PSNR: 16.6211 SSIM: 0.3963 LPIPS: tensor([[[[0.1996]]]])\n",
            "Epoch (12/60) Finished\n",
            "PSNR: 16.5701 SSIM: 0.3829 LPIPS: tensor([[[[0.1969]]]])\n",
            "Epoch (13/60) Finished\n",
            "PSNR: 16.6216 SSIM: 0.3841 LPIPS: tensor([[[[0.1951]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.0100 SSIM: 0.3800\n",
            "Epoch (14/60) Finished\n",
            "PSNR: 16.9198 SSIM: 0.4038 LPIPS: tensor([[[[0.1859]]]])\n",
            "Epoch (15/60) Finished\n",
            "PSNR: 17.2361 SSIM: 0.4248 LPIPS: tensor([[[[0.1971]]]])\n",
            "Epoch (16/60) Finished\n",
            "PSNR: 17.8719 SSIM: 0.4515 LPIPS: tensor([[[[0.2190]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:37<00:00,  7.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.8263 SSIM: 0.4223\n",
            "Epoch (17/60) Finished\n",
            "PSNR: 16.9295 SSIM: 0.4129 LPIPS: tensor([[[[0.1846]]]])\n",
            "Epoch (18/60) Finished\n",
            "PSNR: 16.7770 SSIM: 0.4075 LPIPS: tensor([[[[0.1784]]]])\n",
            "Epoch (19/60) Finished\n",
            "PSNR: 16.9896 SSIM: 0.4045 LPIPS: tensor([[[[0.1939]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.4436 SSIM: 0.3944\n",
            "Epoch (20/60) Finished\n",
            "PSNR: 16.5559 SSIM: 0.3972 LPIPS: tensor([[[[0.1977]]]])\n",
            "Epoch (21/60) Finished\n",
            "PSNR: 16.8665 SSIM: 0.3956 LPIPS: tensor([[[[0.1816]]]])\n",
            "Epoch (22/60) Finished\n",
            "PSNR: 17.4849 SSIM: 0.4393 LPIPS: tensor([[[[0.1926]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.5521 SSIM: 0.4074\n",
            "Epoch (23/60) Finished\n",
            "PSNR: 16.8769 SSIM: 0.3963 LPIPS: tensor([[[[0.2005]]]])\n",
            "Epoch (24/60) Finished\n",
            "PSNR: 17.0829 SSIM: 0.4040 LPIPS: tensor([[[[0.1837]]]])\n",
            "Epoch (25/60) Finished\n",
            "PSNR: 16.8932 SSIM: 0.4067 LPIPS: tensor([[[[0.1755]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:39<00:00,  7.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.2383 SSIM: 0.3876\n",
            "Epoch (26/60) Finished\n",
            "PSNR: 17.0978 SSIM: 0.4130 LPIPS: tensor([[[[0.1873]]]])\n",
            "Epoch (27/60) Finished\n",
            "PSNR: 16.6430 SSIM: 0.3901 LPIPS: tensor([[[[0.1746]]]])\n",
            "Epoch (28/60) Finished\n",
            "PSNR: 16.8644 SSIM: 0.4020 LPIPS: tensor([[[[0.1684]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.3402 SSIM: 0.3926\n",
            "Epoch (29/60) Finished\n",
            "PSNR: 16.2768 SSIM: 0.3813 LPIPS: tensor([[[[0.1818]]]])\n",
            "Epoch (30/60) Finished\n",
            "PSNR: 16.8715 SSIM: 0.4025 LPIPS: tensor([[[[0.1727]]]])\n",
            "Epoch (31/60) Finished\n",
            "PSNR: 16.5766 SSIM: 0.3888 LPIPS: tensor([[[[0.1867]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:37<00:00,  7.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.1888 SSIM: 0.3796\n",
            "Epoch (32/60) Finished\n",
            "PSNR: 16.4728 SSIM: 0.3825 LPIPS: tensor([[[[0.1913]]]])\n",
            "Epoch (33/60) Finished\n",
            "PSNR: 16.7632 SSIM: 0.3962 LPIPS: tensor([[[[0.1636]]]])\n",
            "Epoch (34/60) Finished\n",
            "PSNR: 16.7956 SSIM: 0.3969 LPIPS: tensor([[[[0.1744]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:37<00:00,  7.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.2792 SSIM: 0.3788\n",
            "Epoch (35/60) Finished\n",
            "PSNR: 16.8458 SSIM: 0.4006 LPIPS: tensor([[[[0.1724]]]])\n",
            "Epoch (36/60) Finished\n",
            "PSNR: 16.8033 SSIM: 0.3953 LPIPS: tensor([[[[0.1756]]]])\n",
            "Epoch (37/60) Finished\n",
            "PSNR: 16.8928 SSIM: 0.4054 LPIPS: tensor([[[[0.1766]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.3587 SSIM: 0.3916\n",
            "Epoch (38/60) Finished\n",
            "PSNR: 17.0154 SSIM: 0.4036 LPIPS: tensor([[[[0.1832]]]])\n",
            "Epoch (39/60) Finished\n",
            "PSNR: 17.0248 SSIM: 0.4126 LPIPS: tensor([[[[0.1818]]]])\n",
            "Epoch (40/60) Finished\n",
            "PSNR: 17.0021 SSIM: 0.4076 LPIPS: tensor([[[[0.1662]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:39<00:00,  7.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.3345 SSIM: 0.3870\n",
            "Epoch (41/60) Finished\n",
            "PSNR: 16.8243 SSIM: 0.4009 LPIPS: tensor([[[[0.1728]]]])\n",
            "Epoch (42/60) Finished\n",
            "PSNR: 16.6991 SSIM: 0.3947 LPIPS: tensor([[[[0.1773]]]])\n",
            "Epoch (43/60) Finished\n",
            "PSNR: 17.6378 SSIM: 0.4403 LPIPS: tensor([[[[0.1891]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.6496 SSIM: 0.3959\n",
            "Epoch (44/60) Finished\n",
            "PSNR: 16.8389 SSIM: 0.3991 LPIPS: tensor([[[[0.1742]]]])\n",
            "Epoch (45/60) Finished\n",
            "PSNR: 17.0768 SSIM: 0.4167 LPIPS: tensor([[[[0.1731]]]])\n",
            "Epoch (46/60) Finished\n",
            "PSNR: 16.7482 SSIM: 0.3977 LPIPS: tensor([[[[0.1737]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:37<00:00,  7.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.3244 SSIM: 0.3967\n",
            "Epoch (47/60) Finished\n",
            "PSNR: 16.9373 SSIM: 0.3967 LPIPS: tensor([[[[0.1719]]]])\n",
            "Epoch (48/60) Finished\n",
            "PSNR: 16.8368 SSIM: 0.3988 LPIPS: tensor([[[[0.1735]]]])\n",
            "Epoch (49/60) Finished\n",
            "PSNR: 16.9635 SSIM: 0.4059 LPIPS: tensor([[[[0.1757]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.3281 SSIM: 0.3905\n",
            "Epoch (50/60) Finished\n",
            "PSNR: 16.8989 SSIM: 0.4058 LPIPS: tensor([[[[0.1626]]]])\n",
            "Epoch (51/60) Finished\n",
            "PSNR: 16.9791 SSIM: 0.4063 LPIPS: tensor([[[[0.1614]]]])\n",
            "Epoch (52/60) Finished\n",
            "PSNR: 16.7933 SSIM: 0.3881 LPIPS: tensor([[[[0.1700]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:36<00:00,  8.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.2197 SSIM: 0.3701\n",
            "Epoch (53/60) Finished\n",
            "PSNR: 16.9732 SSIM: 0.4063 LPIPS: tensor([[[[0.1735]]]])\n",
            "Epoch (54/60) Finished\n",
            "PSNR: 17.0001 SSIM: 0.4030 LPIPS: tensor([[[[0.1678]]]])\n",
            "Epoch (55/60) Finished\n",
            "PSNR: 17.1084 SSIM: 0.4053 LPIPS: tensor([[[[0.1783]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:38<00:00,  7.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.6578 SSIM: 0.3993\n",
            "Epoch (56/60) Finished\n",
            "PSNR: 17.0743 SSIM: 0.4078 LPIPS: tensor([[[[0.1715]]]])\n",
            "Epoch (57/60) Finished\n",
            "PSNR: 16.9328 SSIM: 0.3971 LPIPS: tensor([[[[0.1752]]]])\n",
            "Epoch (58/60) Finished\n",
            "PSNR: 16.8304 SSIM: 0.3905 LPIPS: tensor([[[[0.1792]]]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 297/297 [00:37<00:00,  7.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PSNR: 16.2283 SSIM: 0.3730\n",
            "Epoch (59/60) Finished\n",
            "PSNR: 16.8770 SSIM: 0.3985 LPIPS: tensor([[[[0.1750]]]])\n",
            "Epoch (60/60) Finished\n",
            "PSNR: 16.8961 SSIM: 0.3984 LPIPS: tensor([[[[0.1692]]]])\n"
          ]
        }
      ],
      "source": [
        "###### Training ######\n",
        "for epoch in range(epoch, n_epochs):\n",
        "    real_out, fake_out = None, None\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        real_A = Variable(input_A.copy_(batch['A']))\n",
        "        real_B = Variable(input_B.copy_(batch['B']))\n",
        "\n",
        "        ######### (1) forward #########\n",
        "        lf_feature_A, hf_feature_A, fake_B = netG_A2B(real_A) # A2B: lf_feature, hf_feature, rc\n",
        "        hf_feature_A = hf_feature_A.detach()\n",
        "        hf_feature_A.requires_grad = False\n",
        "        lf_feature_A = lf_feature_A.detach()\n",
        "        lf_feature_A.requires_grad = False\n",
        "        hf_feature_recovered_A, lf_feature_recovered_A, recovered_A = netG_B2A(fake_B) # B2A: hf_feature, lf_feature, rc\n",
        "\n",
        "        hf_feature_B, lf_feature_B, fake_A = netG_B2A(real_B)\n",
        "        lf_feature_B = lf_feature_B.detach()\n",
        "        lf_feature_B.requires_grad = False\n",
        "        hf_feature_B = hf_feature_B.detach()\n",
        "        hf_feature_B.requires_grad = False\n",
        "        lf_feature_recovered_B, hf_feature_recovered_B, recovered_B = netG_A2B(fake_A)\n",
        "\n",
        "        # _, _, strong_fake_B = netG_A2B(recovered_A)\n",
        "        # _, _, strong_fake_A = netG_B2A(recovered_B)\n",
        "\n",
        "        # idt_B = F.interpolate(real_B, scale_factor=0.5, mode='nearest')\n",
        "        # _, idt_BB = netG_A2B(idt_B)\n",
        "        # idt_A = F.interpolate(real_A, scale_factor=2, mode='nearest')\n",
        "        # _, idt_AA = netG_B2A(idt_A)\n",
        "\n",
        "\n",
        "        ###### (2) G_A and G_B ######\n",
        "        set_requires_grad([netD_A, netD_B], False)\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        pred_fake = netD_B(fake_B)\n",
        "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
        "\n",
        "        pred_fake = netD_A(fake_A)\n",
        "        loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
        "\n",
        "        # pred_fake = netD_B(strong_fake_B)\n",
        "        # loss_strong_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
        "\n",
        "        # pred_fake = netD_A(strong_fake_A)\n",
        "        # loss_strong_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
        "\n",
        "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10 + criterion_feature(hf_feature_A, hf_feature_recovered_A) + criterion_feature(lf_feature_A, lf_feature_recovered_A) #+ criterion_phase(recovered_A, real_A)\n",
        "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10 + criterion_feature(hf_feature_B, hf_feature_recovered_B) + criterion_feature(lf_feature_B, lf_feature_recovered_B) #+ criterion_phase(recovered_B, real_B)\n",
        "\n",
        "        # 14.6 0.12: loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10 #+ criterion_feature(feature_A, feature_recovered_A) * 2.0 #+ criterion_phase(recovered_A, real_A)\n",
        "\n",
        "        # loss_idt_B = criterion_identity(idt_BB, real_B) * 0.5\n",
        "        # loss_idt_A = criterion_identity(idt_AA, real_A) * 0.5\n",
        "\n",
        "        loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB #+ loss_strong_GAN_A2B + loss_strong_GAN_B2A\n",
        "\n",
        "        loss_G.backward()        \n",
        "        optimizer_G.step()\n",
        "\n",
        "        ###### (3) D_A and D_B ######\n",
        "        set_requires_grad([netD_A, netD_B], True)\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_A(real_A)\n",
        "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
        "        # Fake loss\n",
        "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
        "        pred_fake = netD_A(fake_A.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
        "        # Total loss\n",
        "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
        "        loss_D_A.backward()\n",
        "\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_B(real_B)\n",
        "        loss_D_real = criterion_GAN(pred_real, target_real)      \n",
        "        # Fake loss\n",
        "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
        "        pred_fake = netD_B(fake_B.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
        "        # Total loss\n",
        "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
        "        loss_D_B.backward()\n",
        "\n",
        "        optimizer_D.step()\n",
        "        \n",
        "        ####################################\n",
        "        ####################################\n",
        "\n",
        "        if i == 1:\n",
        "          real_out = real_A\n",
        "          _, _, fake_out = netG_A2B(real_A)\n",
        "      \n",
        "    save_sample(epoch, real_out, \"_input\")\n",
        "    save_sample(epoch, fake_out, \"_output\")\n",
        "\n",
        "    # Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D.step()\n",
        "\n",
        "    if epoch%5==4:\n",
        "      torch.save(netG_A2B.state_dict(), './output/netG_A2B_epoch'+str(epoch+1)+'.pth')\n",
        "    print(\"Epoch (%d/%d) Finished\" % (epoch+1, n_epochs))\n",
        "    _, _, sr_img = netG_A2B(lr_img)\n",
        "    \n",
        "    LPIPS = loss_fn_alex(hr_img.cpu(), sr_img.cpu())\n",
        "    hr_img_cpu = hr_img.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "\n",
        "    yimg = sr_img.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "    psnr = skimage.metrics.peak_signal_noise_ratio(yimg, hr_img_cpu)\n",
        "    ssim = skimage.metrics.structural_similarity(yimg, hr_img_cpu) \n",
        "    print((\"PSNR: %.4f SSIM: %.4f LPIPS:\"%(psnr, ssim)), LPIPS.data)\n",
        "\n",
        "    if epoch%3 == 0:\n",
        "      eval(netG_A2B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MY-xrBjGWrvY"
      },
      "outputs": [],
      "source": [
        "def result_save_sample(epoch, tensor=None, suffix=\"_real\", img=None, img_mode=False):\n",
        "    if tensor != None:\n",
        "      output = tensor.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "      plt.imsave('./results/image_'+str(epoch)+suffix+'.jpeg', output, cmap=\"gray\")\n",
        "    if img_mode:\n",
        "      plt.imsave('./results/image_'+str(epoch)+suffix+'.jpeg', img, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FxE5wdcc7Z3s",
        "outputId": "5b141b57-3cb7-4982-82f6-ecb470959322"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "netG_A2B = torch.load('./output/netG_A2B_epoch40.pth')\n",
        "type(netG_A2B)\n",
        "model = UnetGeneratorA2B(output_nc, input_nc).cuda()\n",
        "model.load_state_dict(netG_A2B, strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDByscuTVLDY"
      },
      "outputs": [],
      "source": [
        "img = dataset[0]['A']\n",
        "x = img.unsqueeze(0).cuda()\n",
        "plt.imshow(img.squeeze(0), \"gray\")\n",
        "result_save_sample(1, tensor=x, suffix=\"_input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q18OiZQRVvBQ"
      },
      "outputs": [],
      "source": [
        "_, _, y = model(x)\n",
        "yimg = y.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "plt.imshow(yimg, \"gray\")\n",
        "result_save_sample(1, tensor=y, suffix=\"_output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sx_rdN_76NQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "upsample = cv2.resize(img.squeeze(0).cpu().numpy(), dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "upsample.shape\n",
        "result_save_sample(1, img_mode=True, img=upsample, suffix=\"_interpolation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kIx7Yy45zMm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img = y.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "# img = y.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "f = np.fft.fft2(img, axes=(-2, -1))\n",
        "fshift = np.fft.fftshift(f)\n",
        "res = np.log(np.abs(fshift))\n",
        "pha = np.angle(fshift)\n",
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(331), plt.imshow(img, 'gray'), plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.subplot(332), plt.imshow(res, 'gray'), plt.title('Fourier Amplitude')\n",
        "plt.axis('off')\n",
        "plt.subplot(333), plt.imshow(pha, 'gray'), plt.title('Fourier Phase')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pqtNTNiriZA"
      },
      "outputs": [],
      "source": [
        "img = dataset[0]['A']\n",
        "x = img.unsqueeze(0).cuda()\n",
        "plt.imshow(img.squeeze(0), \"gray\")\n",
        "_, _, y = model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSMpsnMRs0Od"
      },
      "outputs": [],
      "source": [
        "yimg = y.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "plt.imshow(yimg, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SapXEU8t80Z"
      },
      "outputs": [],
      "source": [
        "class Test_ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_test=None, unaligned=True, mode='test'):\n",
        "        self.transformA = transforms.Compose(transforms_test)\n",
        "        self.transformB = transforms.Compose(transforms_test)\n",
        "\n",
        "        self.unaligned = unaligned\n",
        "\n",
        "        self.files_A = sorted(glob.glob(os.path.join(root, '6x6_256/') + '/*.*'))\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root, '3x3_256/') + '/*.*'))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_A = Image.open(self.files_A[index % len(self.files_A)]).convert('L')\n",
        "        item_A = self.transformA(img_A)\n",
        "\n",
        "        if self.unaligned:\n",
        "            item_B = self.transformB(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]).convert('L'))\n",
        "        else:\n",
        "            item_B = self.transformB(Image.open(self.files_B[index % len(self.files_B)]).convert('L'))\n",
        "\n",
        "        return {'A': item_A, 'B': item_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0z0kn-ttTIp"
      },
      "outputs": [],
      "source": [
        "test_path = \"./test/\"\n",
        "transforms_test = [ \n",
        "                transforms.ToTensor(),\n",
        "                # transforms.Normalize((0.246), (0.170)),\n",
        "                transforms.Normalize((0.5), (0.5)) ]\n",
        "test_dataset = Test_ImageDataset(test_path, transforms_test=transforms_test, unaligned=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2d-44PjtpXF"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "img = test_dataset[2]['A']\n",
        "img = cv2.resize(img.squeeze(0).cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "x = torch.tensor(img).unsqueeze(0).unsqueeze(0).cuda()\n",
        "# plt.imshow(img.squeeze(0), \"gray\")\n",
        "_, _, y = model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfLnAKqHCA1Q"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOKsvPrivP5s"
      },
      "outputs": [],
      "source": [
        "yimg = y.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "plt.imshow(yimg, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsLSjkgKCKwJ"
      },
      "outputs": [],
      "source": [
        "f = np.fft.fft2(yimg, axes=(-2, -1))\n",
        "fshift = np.fft.fftshift(f)\n",
        "res = np.log(np.abs(fshift))\n",
        "pha = np.angle(fshift)\n",
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(331), plt.imshow(yimg, 'gray'), plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.subplot(332), plt.imshow(res, 'gray'), plt.title('Fourier Amplitude')\n",
        "plt.axis('off')\n",
        "plt.subplot(333), plt.imshow(pha, 'gray'), plt.title('Fourier Phase')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAR5512tDS_4"
      },
      "source": [
        "Pick a pair of test data to evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASluKPxzGWDv"
      },
      "outputs": [],
      "source": [
        "netG_A2B = torch.load('./output/netG_A2B_epoch40.pth')\n",
        "type(netG_A2B)\n",
        "model = UnetGeneratorA2B(output_nc, input_nc).cuda()\n",
        "model.load_state_dict(netG_A2B, strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNaXAuyNDSjZ"
      },
      "outputs": [],
      "source": [
        "lr_img = Image.open(\"./test/6x6_256/270_3.png\").convert('L')\n",
        "hr_img = Image.open(\"./test/3x3_256/270_6.png\").convert('L')\n",
        "# lr_img = Image.open(\"./dataset/Colab_centered_OCTA/trainA/STDR403_20181029_101618_Angio (1)_R_001.png\").convert('L')\n",
        "# hr_img = Image.open(\"./dataset/Colab_centered_OCTA/trainB/STDR403_20181029_101802_Angio (1)_R_001.png\").convert('L')\n",
        "T_1 = transforms.Compose([ transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5), (0.5)),\n",
        "                transforms.Resize([128, 128]) ])\n",
        "T_2 = transforms.Compose([ transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5), (0.5))])\n",
        "# lr_img = cv2.resize(np.array(lr_img), dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "# lr_img = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).cuda()\n",
        "lr_img = T_1(lr_img).cuda().unsqueeze(0)\n",
        "hr_img = T_2(hr_img).cuda().unsqueeze(0)\n",
        "# lr_img.size()\n",
        "_, _, sr_img = model(lr_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7h1kFh5EJVv"
      },
      "outputs": [],
      "source": [
        "ximg = lr_img.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "print(ximg.shape)\n",
        "plt.imshow(ximg, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehtYEwRXEG7P"
      },
      "outputs": [],
      "source": [
        "yimg = sr_img.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "plt.imshow(yimg, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NMCtYavERnP"
      },
      "outputs": [],
      "source": [
        "gtimg = hr_img.cpu().detach().numpy().squeeze(0).squeeze(0)\n",
        "plt.imshow(gtimg, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks2aLGjDvgwL"
      },
      "outputs": [],
      "source": [
        "import skimage.metrics\n",
        "print(skimage.metrics.peak_signal_noise_ratio(yimg, gtimg))\n",
        "print(skimage.metrics.structural_similarity(yimg, gtimg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgoTAsZMwBv1"
      },
      "outputs": [],
      "source": [
        "hp_lr_img = high_pass(lr_img[0], i = 8).cpu().detach().numpy()\n",
        "plt.imshow(hp_lr_img, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kjqQo__wFjp"
      },
      "outputs": [],
      "source": [
        "lp_lr_img = low_pass(lr_img[0], i = 20).cpu().detach().numpy()\n",
        "plt.imshow(lp_lr_img*-1, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwJLUj499Ji9"
      },
      "outputs": [],
      "source": [
        "hp_lr_img = high_pass(hr_img[0], i = 1).cpu().detach().numpy()\n",
        "# plt.imshow(hp_lr_img, \"gray\")\n",
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(121), plt.imshow(hp_lr_img, 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(122), plt.imshow(gtimg, 'gray')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX_Yj01e-jwu"
      },
      "outputs": [],
      "source": [
        "hp_lr_img = high_pass(hr_img[0], i = 8).cpu().detach().numpy()\n",
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(131), plt.imshow(hp_lr_img, 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(132), plt.imshow((gtimg+hp_lr_img)/2.0, 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(133), plt.imshow(gtimg, 'gray')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p17kSXnz-Y0W"
      },
      "outputs": [],
      "source": [
        "lp_lr_img = low_pass(hr_img[0], i = 15).cpu().detach().numpy()\n",
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(131), plt.imshow(lp_lr_img*-1, 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(132), plt.imshow((gtimg+lp_lr_img)/2.0, 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(133), plt.imshow(gtimg, 'gray')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jaeTVrzWae"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSrWyz5t-d2A"
      },
      "outputs": [],
      "source": [
        "import skimage.metrics\n",
        "print(skimage.metrics.peak_signal_noise_ratio(yimg, gtimg))\n",
        "print(skimage.metrics.structural_similarity(yimg, gtimg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKMXdTjrxsBP"
      },
      "outputs": [],
      "source": [
        "lr_img = Image.open(\"./test/6x6_256/270_3.png\").convert('L')\n",
        "hr_img = Image.open(\"./test/3x3_256/270_6.png\").convert('L')\n",
        "lr_img = T_1(lr_img)\n",
        "hr_img = T_2(hr_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKQZG5b9ywEQ"
      },
      "outputs": [],
      "source": [
        "input = lr_img.cuda().unsqueeze(0)\n",
        "_, _, output = model(input)\n",
        "output = output.cpu().detach().numpy().squeeze(0).squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT20o4xFxvfz"
      },
      "outputs": [],
      "source": [
        "ximg = lr_img.detach().numpy().squeeze(0)\n",
        "yimg = hr_img.detach().numpy().squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBHuKIMUx9nz"
      },
      "outputs": [],
      "source": [
        "srimg = cv2.resize(ximg, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zhXqnicym9X"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(121), plt.imshow(yimg, 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(122), plt.imshow(srimg, 'gray')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlcKTrI1ydE4"
      },
      "outputs": [],
      "source": [
        "print(skimage.metrics.peak_signal_noise_ratio(yimg, srimg))\n",
        "print(skimage.metrics.structural_similarity(yimg, srimg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vwM51B4yk-v"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(121), plt.imshow(yimg, 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(122), plt.imshow(output, 'gray')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XseKwNxKygOK"
      },
      "outputs": [],
      "source": [
        "print(skimage.metrics.peak_signal_noise_ratio(yimg, (output+srimg)/2.0))\n",
        "print(skimage.metrics.structural_similarity(yimg, output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUveTrDozErk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "OCTA_Cyclegan_v7.3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}